# Monolingual BERT models

This directory contains the code for training monolingual BERT models on monolingual alternative sets.

The files containing the monolingual alternative sentences should be placed in `data/`.

I had compared the following pretrained models
- `bert-base-multilingual-cased`
- `bert-base-uncased`
- `bert-large-uncased-whole-word-masking`
- `hfl/chinese-bert-wwm-ext`
- `hfl/chinese-bert-wwm`
